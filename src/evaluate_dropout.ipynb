{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "#\n",
    "#\n",
    "# start Tensorboard with CMD-Shft-P Python:Launch TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import AxesImage, NonUniformImage\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from scipy.interpolate import interp1d\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import uuid\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),'..'))\n",
    "from lib import find_nearest_index, FigureSize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_base_dir = \"./logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OUTPUTS = 26 # no. of peaks\n",
    "NUM_BATCHES = 32\n",
    "NUM_EPOCHS = 100\n",
    "IMAGE_SIZE = 32\n",
    "#NUM_TRAIN_LABELS = 2600 # use outpuf of load_images()\n",
    "\n",
    "TRAIN_DATA_PATH= os.path.join(os.getcwd(), '..','data','train')\n",
    "TEST_DATA_PATH = os.path.join(os.getcwd(), '..','data','test')\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "MODEL_PATH = os.path.join(os.getcwd(), '..','data','model'+timestamp+'.h5')\n",
    "\n",
    "TESTDATA_TRAINDATA_RATIO = 20./80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEON_REFERENCE_FILE = os.path.join(os.getcwd(),'..','data','ref','NIST','Ne','neon-exported.csv')\n",
    "WAVELENGTHS_MIN, WAVELENGTHS_MAX = 4000, 9000\n",
    "WINDOW = 512\n",
    "STEPSIZE_MIN, STEPSIZE_MAX, STEPSIZE_N = 0.5, 1.5, 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_NUMBER = math.pow(10,int(math.log10(STEPSIZE_N *NUM_OUTPUTS))+1)\n",
    "print (RANDOM_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SSIM = 0.6\n",
    "PERCENT = 1.0\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(min_scale, max_scale):\n",
    "    return random.random()*(max_scale - min_scale)+0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss():\n",
    "    return random.gauss(1.0,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "        \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Reshape((IMAGE_SIZE,IMAGE_SIZE,1),input_shape=(IMAGE_SIZE,IMAGE_SIZE,1)))\n",
    "    # \n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2))) \n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2))) \n",
    "    # prediction part        \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1024))\n",
    "    model.add(tf.keras.layers.Dropout(DROPOUT))\n",
    "    # model.add(tf.keras.layers.Dense(NUM_TRAIN_LABELS, activation='softmax')) \n",
    "    model.add(tf.keras.layers.Dense(NUM_OUTPUTS, activation='softmax')) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img =cv2.imread(image_path)\n",
    "    img=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.reshape([IMAGE_SIZE, IMAGE_SIZE,1])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    dir_index = 0\n",
    "    for directory in sorted(os.listdir(path)):\n",
    "        if(directory.startswith('.') == False):\n",
    "\n",
    "            \n",
    "            for filename in sorted(os.listdir(os.path.join(path, directory))):\n",
    "\n",
    "                if(filename.startswith('.') == False):\n",
    "                        \n",
    "                        image_path = os.path.join(path, directory,filename    )\n",
    "                        img =cv2.imread(image_path)\n",
    "                        img=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                        img = img.reshape([IMAGE_SIZE, IMAGE_SIZE,1])\n",
    "                        train_images.append(img)\n",
    "                train_labels.append(dir_index)\n",
    "            dir_index = dir_index + 1\n",
    "\n",
    "    print(len(train_images),train_labels)\n",
    "    return np.array(train_images)/255, np.array(tf.keras.utils.to_categorical(train_labels,NUM_OUTPUTS))#len(train_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_probabilities(probs):\n",
    "    count_0 = (0.9 <= probs).sum()\n",
    "    count_1 = ((0.1 < probs) & (probs < 0.9)).sum()\n",
    "    count_2 = ((0.01 <= probs) & (probs < 0.1)).sum()\n",
    "    count_e10 = ((1.0e-10 <= probs) & (probs < 0.01 )).sum()\n",
    "    count_e20 = ((1.0e-20 <= probs) & (probs < 1e-10 )).sum()\n",
    "    count_e30 = ((1.0e-30 <= probs) & (probs < 1e-20 )).sum()\n",
    "    count_e40 = ((1.0e-40 <= probs) & (probs < 1e-30 )).sum()\n",
    "    counts = count_0 + count_1 + count_2 + count_e10 + count_e20 + count_e30 + count_e40\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test auf Dataset\n",
    "def test_dataset(model, labels_decoded, path, image_size=IMAGE_SIZE, resultfile=None):\n",
    "    correct_matches = 0\n",
    "    wrong_matches = 0\n",
    "    result_table = PrettyTable()\n",
    "    result_table.field_names = [\"Datei \", \"Ist\", \"Dekodiert\", \"Match?\"]\n",
    "\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "\n",
    "        if(filename.startswith('.') == False):\n",
    "\n",
    "            current_wavelength = filename[0:7]\n",
    "            #print (current_wavelength)\n",
    "            image_path = os.path.join(path,filename)\n",
    "                \n",
    "            test_image = load_image(image_path)\n",
    "            predictions = model.predict(test_image.reshape((1,IMAGE_SIZE,IMAGE_SIZE,1)), verbose=0)\n",
    "            counts = count_probabilities(predictions)\n",
    "                \n",
    "            index_max_predictions = np.argmax(predictions)\n",
    "            #print(predictions)\n",
    "            #print('index_max_predictions:',index_max_predictions, current_wavelength, labels_decoded[index_max_predictions])\n",
    "            decode_wavelength = labels_decoded[index_max_predictions]\n",
    "            # Passt oder nicht?\n",
    "            if str.upper(current_wavelength) == str.upper(decode_wavelength):\n",
    "                mark = \"✅ {:02d}\".format(counts)\n",
    "                correct_matches += 1 \n",
    "            else:\n",
    "                mark = \"❌ {:02d}\".format(counts)\n",
    "                wrong_matches += 1\n",
    "            result_table.add_row([image_path, current_wavelength, decode_wavelength, mark ])\n",
    "\n",
    "\n",
    "    #print(result_table)\n",
    "    if resultfile is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        resultfile = \"result_\"+timestamp+\".txt\"\n",
    "    with open(resultfile,\"w\") as result_f:\n",
    "        result_f.write(str(result_table))\n",
    "    return correct_matches, wrong_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_reference_file = os.path.join(NEON_REFERENCE_FILE)\n",
    "positions = {'wavelength':2, 'intensity':6, 'selector':1}\n",
    "selector = '1'\n",
    "intensity_limit = 1.0\n",
    "neon_wavelengths = []\n",
    "neon_intensities = []\n",
    "\n",
    "with open(neon_reference_file,'r') as neon_f:\n",
    "    for line in neon_f:\n",
    "        if line.startswith('#'):\n",
    "            pass\n",
    "        else:\n",
    "            tokens =  line.split(';')\n",
    "            #print (tokens)\n",
    "            try:\n",
    "                if selector in tokens[positions['selector']] :\n",
    "                    \n",
    "                    neon_wavelength, neon_intensity = float(tokens[positions['wavelength']]), float(tokens[positions['intensity']])\n",
    "                    \n",
    "                    if neon_intensity > intensity_limit :\n",
    "                        \n",
    "                        neon_wavelengths.append(neon_wavelength)\n",
    "                        neon_intensities.append(neon_intensity)\n",
    "                        \n",
    "                        print (\"{:8.3f} {:10.0f}\".format(neon_wavelength, neon_intensity, ))\n",
    "                    \n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "print (len(neon_wavelengths), len(neon_intensities))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_reference_file = 'linetable-NE.csv'\n",
    "positions = {'wavelength':0, 'intensity':2}\n",
    "neon_wavelengths = []\n",
    "neon_intensities = []\n",
    "\n",
    "with open(neon_reference_file,'r') as neon_f:\n",
    "    for line in neon_f:\n",
    "        if line.startswith('#'):\n",
    "            pass\n",
    "        else:\n",
    "            tokens =  line.split(';')\n",
    "            print (tokens)\n",
    "            try:\n",
    "                neon_wavelength, neon_intensity = float(tokens[positions['wavelength']]), float(tokens[positions['intensity']])\n",
    "                neon_wavelengths.append(neon_wavelength)\n",
    "                neon_intensities.append(neon_intensity)\n",
    "                \n",
    "                print (\"{:8.3f} {:10.0f}\".format(neon_wavelength, neon_intensity, ))\n",
    "                \n",
    "                        \n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "print (len(neon_wavelengths), len(neon_intensities))\n",
    "#\n",
    "# print (neon_wavelengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = FigureSize.NARROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelengths = np.array(range(WAVELENGTHS_MIN,WAVELENGTHS_MAX,1))*1.0\n",
    "intensities = wavelengths * 0.0\n",
    "sigma = 4.0\n",
    "k = -2*sigma*sigma\n",
    "r = math.sqrt(2*math.pi*sigma*sigma)\n",
    "\n",
    "for neon_w, neon_i, in zip(neon_wavelengths, neon_intensities):\n",
    "\n",
    "    _e = (wavelengths - neon_w)*(wavelengths- neon_w) / k\n",
    "    intensities = intensities + np.exp(_e) * neon_i\n",
    "        \n",
    "\n",
    "xlim = [wavelengths[1], wavelengths[-2]]\n",
    "_i1 = find_nearest_index(wavelengths,xlim[0])\n",
    "_i2 = find_nearest_index(wavelengths,xlim[1])\n",
    "#print (_i1[0], _i2[0])\n",
    "max_i = intensities[_i1:_i2].max()\n",
    "normalized_intensities = intensities / max_i\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(wavelengths, normalized_intensities)\n",
    "for _nw, _ni in zip(neon_wavelengths, neon_intensities):\n",
    "    plt.text(_nw,_ni/max_i,\"{:6.1f}\".format(_nw), rotation=90, horizontalalignment='center')\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(0,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelengths = np.array(range(WAVELENGTHS_MIN,WAVELENGTHS_MAX,1))*1.0\n",
    "pixels = np.array(range(0,len(wavelengths)))\n",
    "intensities = wavelengths * 0.0\n",
    "sigma = 4.0\n",
    "k = -2*sigma*sigma\n",
    "r = math.sqrt(2*math.pi*sigma*sigma)\n",
    "\n",
    "for neon_w, neon_i, in zip(neon_wavelengths, neon_intensities):\n",
    "\n",
    "    _e = (wavelengths - neon_w)*(wavelengths- neon_w) / k\n",
    "    intensities = intensities + np.exp(_e) * neon_i\n",
    "\n",
    "xlim = [wavelengths[1], wavelengths[-2]]\n",
    "_i1 = find_nearest_index(wavelengths,xlim[0])\n",
    "_i2 = find_nearest_index(wavelengths,xlim[1])\n",
    "max_i = intensities[_i1:_i2].max()\n",
    "normalized_intensities = intensities / max_i\n",
    "\n",
    "ny = WINDOW\n",
    "nx = len(normalized_intensities)\n",
    "\n",
    "twod = np.zeros((ny, nx))\n",
    "for i in range(ny):\n",
    "    twod[i] = normalized_intensities*-1+1.0\n",
    "\n",
    "ylim = [0,ny]\n",
    "\n",
    "n_cols = 5\n",
    "n_rows = math.ceil(len(neon_wavelengths)/n_cols)\n",
    "\n",
    "window = WINDOW\n",
    "window_h = int(window/2)\n",
    "image_size = [IMAGE_SIZE,IMAGE_SIZE]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = FigureSize.LARGE\n",
    "fig, axes = plt.subplots(n_rows, n_cols,sharex=False, sharey=False)\n",
    "\n",
    "i_row = 0\n",
    "i_col = 0\n",
    "\n",
    "for index in range(0,len(neon_wavelengths)):\n",
    "    neon_w = neon_wavelengths[index]\n",
    "    \n",
    "    _xlim = [\n",
    "            find_nearest_index(wavelengths,neon_w)-window_h,\n",
    "            find_nearest_index(wavelengths,neon_w)+window_h,\n",
    "    ]\n",
    "    _ylim = [0,ny]\n",
    "    _twod = twod[_ylim[0]:_ylim[1], _xlim[0]:_xlim[1]]\n",
    "    wavelength_text = str(int(neon_w*100)/100)\n",
    "    print(index, neon_w, wavelength_text, _xlim)\n",
    "    \n",
    "\n",
    "    if n_cols == 1:\n",
    "        axes[i_row].imshow(_twod, cmap='gray')\n",
    "        axes[i_row].set_title(wavelength_text)\n",
    "        axes[i_row].set_ylim(_ylim)\n",
    "        axes[i_row].set_xlim(0,_xlim[1]-_xlim[0])\n",
    "        print (i_row, wavelength_text, _xlim)\n",
    "        i_row += 1\n",
    "    else:\n",
    "        axes[i_row, i_col].imshow(_twod, cmap='gray')\n",
    "        axes[i_row, i_col].set_title(wavelength_text)\n",
    "        axes[i_row, i_col].set_ylim(_ylim)\n",
    "        axes[i_row, i_col].set_xlim(0,_xlim[1]-_xlim[0])\n",
    "\n",
    "        i_col += 1\n",
    "        if i_col >= n_cols:\n",
    "            i_col = 0\n",
    "            i_row +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neon_w in neon_wavelengths:\n",
    "    wavelength_text = str(int(neon_w*100)/100)\n",
    "    p = os.path.join(TRAIN_DATA_PATH, wavelength_text)\n",
    "    try:\n",
    "        shutil.rmtree(p, ignore_errors=True)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print (\"skipping \"+p)\n",
    "    os.mkdir(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 4.0\n",
    "k = -2*sigma*sigma\n",
    "r = math.sqrt(2*math.pi*sigma*sigma)\n",
    "\n",
    "xlim = [wavelengths[1], wavelengths[-2]]\n",
    "ny = WINDOW\n",
    "nx = len(normalized_intensities)\n",
    "ylim = [0,ny]\n",
    "\n",
    "\n",
    "window = WINDOW\n",
    "window_h = int(window/2)\n",
    "image_size = [IMAGE_SIZE,IMAGE_SIZE]\n",
    "\n",
    "filename_counter = [0] * len (neon_wavelengths)\n",
    "\n",
    "min_stepsize = STEPSIZE_MIN\n",
    "max_stepsize = STEPSIZE_MAX\n",
    "n_steps = STEPSIZE_N\n",
    "d_step = (max_stepsize - min_stepsize)/n_steps\n",
    "\n",
    "stepsizes = np.array(range(n_steps))*d_step+min_stepsize\n",
    "#print (stepsizes)\n",
    "nbins = WAVELENGTHS_MAX - WAVELENGTHS_MIN\n",
    "for stepsize in stepsizes:\n",
    "    wavelengths = np.array(range(nbins))*stepsize+WAVELENGTHS_MIN #   np.array(range(4000,9000,stepsize))*1.0\n",
    "    \n",
    "    intensities = np.zeros(nbins)\n",
    "\n",
    "    for neon_w, neon_i, in zip(neon_wavelengths, neon_intensities):\n",
    "        _neon_w = neon_w+random.gauss(0,1.5)\n",
    "        _e = (wavelengths - _neon_w)*(wavelengths- _neon_w) / k\n",
    "        intensities = intensities + np.exp(_e) * neon_i*gauss() # scale(0.9,1.0)\n",
    "\n",
    "    max_i = intensities[1:-2].max()\n",
    "    normalized_intensities = intensities / max_i\n",
    "\n",
    "    twod = np.zeros((ny, nx))\n",
    "    for i in range(ny):\n",
    "        twod[i] = normalized_intensities*-1+1.0\n",
    "\n",
    "\n",
    "    for index in range(0,len(neon_wavelengths)):\n",
    "        neon_w = neon_wavelengths[index]\n",
    "        \n",
    "        _xlim = [\n",
    "            find_nearest_index(wavelengths,neon_w)-window_h,\n",
    "            find_nearest_index(wavelengths,neon_w)+window_h,\n",
    "        ]\n",
    "        _ylim = [0,ny]\n",
    "        _twod = twod[_ylim[0]:_ylim[1], _xlim[0]:_xlim[1]]\n",
    "        wavelength_text = str(int(neon_w*100)/100)\n",
    "        \n",
    "        res = cv2.resize(np.uint8(_twod * 255), dsize=(IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\n",
    "        img = Image.fromarray(res)\n",
    "\n",
    "        prob = random.random()\n",
    "        \n",
    "        if prob < TESTDATA_TRAINDATA_RATIO:\n",
    "            p = os.path.join(TEST_DATA_PATH,'{:s}.{:06d}.BMP'.format(wavelength_text,filename_counter[index]))\n",
    "        else:\n",
    "            p = os.path.join(TRAIN_DATA_PATH,wavelength_text,'{:s}.{:06d}.BMP'.format(wavelength_text,filename_counter[index]))\n",
    "\n",
    "        img.save(p, format='BMP')\n",
    "        \n",
    "        filename_counter[index] += 1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Avoid training with images having a very high similarity\n",
    "\n",
    "Method: Naively, remove any images with a very high similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vergleich.txt', 'w') as vergleich_f:\n",
    "#     train_path = Path(TRAIN_DATA_PATH)\n",
    "#     train_files = list(train_path.glob('**/*.BMP'))\n",
    "#     train_files_l = len(train_files)\n",
    "#     part = train_files_l * PERCENT/ 100\n",
    "#     now0 = datetime.now()\n",
    "#     report = []\n",
    "#     i = 0\n",
    "#     j = 0\n",
    "#     while len(train_files) > 0:\n",
    "#         if int(i%part)==0:\n",
    "#             print(\"{}: {} completed\".format(datetime.now(),int(i/part*PERCENT)))\n",
    "     \n",
    "#         l_file = train_files.pop()\n",
    "#         j = 0\n",
    "#         for r_file in train_files:\n",
    "#             j += 1\n",
    "# #            if l_file.parts[-2] != r_file.parts[-2]:\n",
    "# #                pass\n",
    "# #                if l_file.exists() and r_file.exists():\n",
    "# #                    pass\n",
    "#                     #l_img = load_image(str(l_file))\n",
    "#                     #r_img = load_image(str(r_file))\n",
    "#                     #ssim =tf.image.ssim(l_img, r_img, max_val=255, filter_size=11, filter_sigma=1.5, k1=0.01, k2=0.03)\n",
    "\n",
    "\n",
    "#             with Image.open(str(l_file)) as im1:\n",
    "#                 bytes1 = im1.tobytes()\n",
    "#             #ref_rec = list(bytes1)[0:31]\n",
    "\n",
    "#             with Image.open(str(r_file)) as im2:\n",
    "#                 bytes2 = im2.tobytes()\n",
    "#             #inp_rec = list(bytes2)[0:31]\n",
    "\n",
    "    #         #ref_time = np.correlate(ref_rec,ref_rec)\n",
    "    #         #inp_time = np.correlate(ref_rec,inp_rec)\n",
    "    #         #diff_time = abs(ref_time-inp_time)\n",
    "                    \n",
    "    #         diff_time = abs( np.correlate(list(bytes1)[0:31],list(bytes1)[0:31]) - np.correlate(list(bytes1)[0:31],list(bytes2)[0:31]))\n",
    "\n",
    "    #                 #vergleich_f.write(\"{} {} {} \\n\".format(\n",
    "    #                 #        l_file.parts[-2:], \n",
    "    #                 #        r_file.parts[-2:], \n",
    "    #                 #        diff_time\n",
    "    #                 #        )\n",
    "    #                 #)\n",
    "    #                 #if ssim > MAX_SSIM:\n",
    "    #                 #    l_file.unlink()\n",
    "    #                 #    r_file.unlink()\n",
    "    #         report.append({'l_file':l_file.parts[-1], 'r_file': r_file.parts[-1], 'ssim': diff_time})\n",
    "\n",
    "\n",
    "    #     i += 1\n",
    "    # now1 = datetime.now()\n",
    "    # print(now1 - now0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('report.txt','w') as report_out:\n",
    "#    for r in report:\n",
    "#        report_out.write(str(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels  = load_images(TRAIN_DATA_PATH)\n",
    "\n",
    "\n",
    "train_images, train_labels  = shuffle(train_images,train_labels,random_state=21)\n",
    "print (len(train_labels))\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(train_images, train_labels, test_size = 0.2, random_state = 0)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "\n",
    "for i in range(6,7):\n",
    "    DROPOUT = 0.1 * i\n",
    "    model = build_model()\n",
    "    #model.summary()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics = [\"accuracy\"])\n",
    "    tf_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\",histogram_freq=5)\n",
    "    model.fit(xTrain,yTrain, epochs=NUM_EPOCHS,batch_size=NUM_BATCHES, callbacks=[tf_callback], verbose=1,validation_split=0.2)\n",
    "\n",
    "    results = model.evaluate(xTest,yTest,verbose=1)\n",
    "    print(\"--- Ergebnisse {} ----\".format(TRAIN_DATA_PATH))\n",
    "    print('Evaluation / Loss {}, Acc:{}'.format(results[0],results[1]))\n",
    "\n",
    "    labels_decoded = []\n",
    "    for directory in sorted(os.listdir(TRAIN_DATA_PATH)):\n",
    "        if(directory.startswith('.') == False):\n",
    "            labels_decoded.append(directory)\n",
    "\n",
    "    corrects, wrongs = test_dataset(model, labels_decoded, TEST_DATA_PATH, resultfile='results_DROPOUT_{:04.2f}.txt'.format(DROPOUT))\n",
    "    print(\"DROPOUT:\", DROPOUT, corrects, wrongs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d499eb6db3777fdf4e4e943312bd37ba260a38bd75a3963efef353b7cf2eca9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
